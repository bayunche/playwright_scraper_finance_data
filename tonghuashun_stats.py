# filename: tonghuashun_stats.py
import asyncio
from playwright.async_api import async_playwright
import re # å¯¼å…¥reæ¨¡å—
from bs4 import BeautifulSoup # å¯¼å…¥BeautifulSoup
# from playwright_stealth import stealth_async # ç§»é™¤æ­¤è¡Œ

URL = "https://q.10jqka.com.cn/"

async def get_top3_rows(page):
    """æå–å½“å‰æ’åºä¸‹çš„å‰ä¸‰è¡Œè‚¡ç¥¨æ•°æ®"""
    rows = await page.query_selector_all("table.m-table tbody tr")
    top3 = []
    for row in rows[:3]:
        tds = await row.query_selector_all("td")
        name = await tds[2].inner_text()
        code = await tds[1].inner_text()
        price = await tds[3].inner_text()
        change_percent = await tds[4].inner_text()
        top3.append({
            "code": code,
            "name": name,
            "price": price,
            "change_percent": change_percent
        })
    return top3

async def scrape_today():
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(headless=True)  # å…³é—­æ— å¤´æ¨¡å¼ä»¥ä¾¿è°ƒè¯•
        page = await browser.new_page()

        # éšè—çˆ¬è™«æŒ‡çº¹
        # ç§»é™¤æ‰‹åŠ¨è®¾ç½®User-Agentå’Œviewport_sizeï¼Œç”±stealth_asyncå¤„ç†
        await page.set_extra_http_headers({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        })
        await page.set_viewport_size({"width": 1920, "height": 1080})

        # æ·»åŠ ç”¨æˆ·æä¾›çš„init_script
        init_script = """
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            Object.defineProperty(navigator, 'platform', { get: () => 'MacIntel' });
            window.chrome = {
                runtime: {},
                // æ·»åŠ å…¶ä»– Chrome å¯¹è±¡å±æ€§
            }
        """
        await page.add_init_script(script=init_script)

        # æ›´å¿«æŠ“å–ï¼šé˜»æ­¢å›¾ç‰‡ã€å­—ä½“ç­‰èµ„æºåŠ è½½
        # å¢åŠ é¡µé¢åŠ è½½æ£€æŸ¥
        await page.goto(URL, wait_until="domcontentloaded")
        
        result = {}

        try:
            all_stocks = [] # åœ¨tryå—å¼€å§‹æ—¶åˆå§‹åŒ–all_stocks
            # ç­‰å¾…hcharts-listå…ƒç´ å¯è§
            await page.wait_for_timeout(5000) # å¢åŠ ç­‰å¾…æ—¶é—´
            # è·å–hcharts-listçš„HTMLå†…å®¹
            hcharts_list_html = await page.locator('.hcharts-list').inner_html(timeout=120000) # å¢åŠ è¶…æ—¶æ—¶é—´
            soup = BeautifulSoup(hcharts_list_html, 'lxml')

            # ç­‰å¾…è¡¨æ ¼çš„tbodyä¸­è‡³å°‘æœ‰ä¸€è¡Œæ•°æ®åŠ è½½å®Œæˆ
            # await page.wait_for_selector('table.m-table.m-pager-table tbody tr', timeout=60000) # å¢åŠ ç­‰å¾…è¡¨æ ¼è¡Œçš„è¶…æ—¶æ—¶é—´
            await page.wait_for_timeout(5000) # å¢åŠ ç­‰å¾…æ—¶é—´


            # æ¶¨è·Œåˆ†å¸ƒ (ä¸Šæ¶¨/ä¸‹è·Œå®¶æ•°)
            rise_span_dist = soup.select_one('div.item:has(h3:contains("æ¶¨è·Œåˆ†å¸ƒ")) p.detail span.c-rise')
            if rise_span_dist:
                match = re.search(r'ä¸Šæ¶¨ï¼š(\d+)åª', rise_span_dist.get_text())
                result['riseCount'] = int(match.group(1)) if match else 0
            else:
                result['riseCount'] = 0

            fall_span_dist = soup.select_one('div.item:has(h3:contains("æ¶¨è·Œåˆ†å¸ƒ")) p.detail span.c-fall')
            if fall_span_dist:
                match = re.search(r'ä¸‹è·Œï¼š(\d+)åª', fall_span_dist.get_text())
                result['fallCount'] = int(match.group(1)) if match else 0
                result['downCount'] = result['fallCount']  # æ·»åŠ ä¸fallCountç›¸åŒçš„å­—æ®µ
                result['upCount'] = result['riseCount']  # æ·»åŠ ä¸riseCountç›¸åŒçš„å­—æ®µ
            else:
                result['fallCount'] = 0
                result['downCount'] = 0  # å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿåˆå§‹åŒ–
                result['upCount'] = 0  # å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿåˆå§‹åŒ–

            # æ¶¨è·Œåœ (æ¶¨åœ/è·Œåœå®¶æ•°)
            limit_up_span = soup.select_one('div.item:has(h3:contains("æ¶¨è·Œåœ")) p.detail span.c-rise')
            if limit_up_span:
                match = re.search(r'æ¶¨åœï¼š(\d+)åª', limit_up_span.get_text())
                result['limitUpCount'] = int(match.group(1)) if match else 0
            else:
                result['limitUpCount'] = 0

            limit_down_span = soup.select_one('div.item:has(h3:contains("æ¶¨è·Œåœ")) p.detail span.c-fall')
            if limit_down_span:
                match = re.search(r'è·Œåœï¼š(\d+)åª', limit_down_span.get_text())
                result['limitDownCount'] = int(match.group(1)) if match else 0
            else:
                result['limitDownCount'] = 0

            # æ˜¨æ—¥æ¶¨åœä»Šæ—¥æ”¶ç›Š
            yesterday_limit_up_profit_span = soup.select_one('div.item:has(h3:contains("æ˜¨æ—¥æ¶¨åœä»Šæ—¥æ”¶ç›Š")) p.detail span.c-rise')
            if yesterday_limit_up_profit_span:
                match = re.search(r'ä»Šæ”¶ç›Šï¼š([\d.]+)%', yesterday_limit_up_profit_span.get_text())
                result['yesterdayLimitUpProfit'] = float(match.group(1)) if match else 0.0
            else:
                result['yesterdayLimitUpProfit'] = 0.0

            # æ¶¨åœå‰ä¸‰è‚¡ç¥¨å’Œè·Œåœå‰ä¸‰è‚¡ç¥¨
            # è·å–æ¶¨å¹…å‰ä¸‰è‚¡ç¥¨
            result['limitUpTop3'] = await get_top3_rows(page)
            result['limitUpList'] = result['limitUpTop3'] # æ·»åŠ åˆ«å

            # ç‚¹å‡»â€œæ¶¨è·Œå¹…â€åˆ—å¤´ï¼Œåˆ‡æ¢ä¸ºå‡åºæ’åº
            await page.evaluate('document.querySelector("a[field=\'zdf\']").click()')
            
            await page.wait_for_timeout(1000) # ç­‰å¾…æ’åºå®Œæˆ

            # å†è·å–è·Œå¹…å‰ä¸‰è‚¡ç¥¨
            result['limitDownTop3'] = await get_top3_rows(page)
            result['limitDownList'] = result['limitDownTop3'] # æ·»åŠ åˆ«å

        except Exception as e:
            print(f"çˆ¬å–æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # åˆå§‹åŒ–æ‰€æœ‰ç»“æœä¸ºé»˜è®¤å€¼ï¼Œä»¥é˜²ä»»ä½•éƒ¨åˆ†çˆ¬å–å¤±è´¥
            result['limitUpCount'] = 0
            result['limitDownCount'] = 0
            result['riseCount'] = 0
            result['fallCount'] = 0
            result['downCount'] = 0  # æ·»åŠ ä¸fallCountç›¸åŒçš„å­—æ®µ
            result['upCount'] = 0  # æ·»åŠ ä¸riseCountç›¸åŒçš„å­—æ®µ
            result['yesterdayLimitUpProfit'] = 0.0 # æ–°å¢å­—æ®µåˆå§‹åŒ–
            result['limitUpTop3'] = [] # ç¡®ä¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿè¢«åˆå§‹åŒ–
            result['limitUpList'] = [] # æ·»åŠ ä¸limitUpTop3ç›¸åŒçš„å­—æ®µ
            result['limitDownTop3'] = [] # ç¡®ä¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿè¢«åˆå§‹åŒ–
            result['limitDownList'] = [] # æ·»åŠ ä¸limitDownTop3ç›¸åŒçš„å­—æ®µ

        await browser.close()
        return result

async def main():
    result = await scrape_today()
    print("ğŸ“ˆ é‡‘èç•Œ ä»Šæ—¥ç»Ÿè®¡ï¼š")
    print(f" æ¶¨åœå®¶æ•°ï¼š{result['limitUpCount']}")
    print(f" è·Œåœå®¶æ•°ï¼š{result['limitDownCount']}")
    print(f" ä»Šæ—¥ä¸‹è·Œï¼š{result['fallCount']} å®¶")
    print(f" ä»Šæ—¥ä¸Šæ¶¨ï¼š{result['riseCount']} å®¶")
    
    print("\nğŸ“ˆ æ¶¨å¹…å‰ä¸‰ï¼š")
    for stock in result['limitUpTop3']:
        print(f'{stock["name"]}ï¼ˆ{stock["code"]}ï¼‰: {stock["change_percent"]}%')

    print("\nğŸ“‰ è·Œå¹…å‰ä¸‰ï¼š")
    for stock in result['limitDownTop3']:
        print(f'{stock["name"]}ï¼ˆ{stock["code"]}ï¼‰: {stock["change_percent"]}%')
    
    print(f"\næ˜¨æ—¥æ¶¨åœä»Šæ—¥æ”¶ç›Šï¼š{result['yesterdayLimitUpProfit']}%")

if __name__ == "__main__":
    asyncio.run(main())
